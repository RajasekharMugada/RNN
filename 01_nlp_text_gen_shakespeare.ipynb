{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_cglLP9qkNJ"
   },
   "source": [
    "# Text generation using NLP for Shakespeare plays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vq0gnyaMqkNM"
   },
   "source": [
    "#### For opensource text corpus https://www.gutenberg.org/  is a good reference\n",
    "#### Refer blog for more details about RNNs : http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "## Steps \n",
    "\n",
    "* STEP 1 : Load text\n",
    "    - Load text corpus. Ideally the text corpus should be in millions of characters for reliable predictions\n",
    "    - Understand the characters and style of the text\n",
    "    - Find out vocabulary length for the text input\n",
    "    \n",
    "* STEP 2 : Text processing\n",
    "    - Vectorize the character symbols(vocabulory). (In this example there are around 84 distinct characters)\n",
    "    - Encode the entire text   \n",
    "    \n",
    "* STEP 3 : Sequence/Batch generation\n",
    "    - Identify any patter in the text and choose a time sequence length (here 120 characters)\n",
    "    - Convert the text in to tensorflow dataset sequences (batch_size of 128, dataset size = 128,120, 84)\n",
    "    - Shuffle the batches so that the consecutive batches are generated from different parts of the text\n",
    "    \n",
    "* STEP 4: Model creation\n",
    "    - Define loss function - sparse_categorical_loss since the output labels large and one-hot encoded\n",
    "    - Create model with Embedding layer -> GRU -> Dense\n",
    "    - Embedding vector size can be choosen based on the number of encoding the text.(Taking embeddings size 64) \n",
    "    - Choose large number of GRU units (around 1024 for this type of examples)\n",
    "    - Dense layer out put size should be of vocab size(84)\n",
    "    - (Epochs are around 30 in this examples)\n",
    " \n",
    "* STEP 5: Model training \n",
    "    - Train the model and save its weigths for re-use\n",
    "    - Since the trainable parameters in this example are very huge (~3.5 million), use of GPU or google colab notes is recommended\n",
    "\n",
    "* STEP 6 : Text generator\n",
    "    - Load model weights and create a model which takes single text sequence at a time (batch_size of 1)\n",
    "    - Predict model output and choose a single output character based on the probability score and temperature from possible outputs (in this case 84) \n",
    "    - Iterate and keep appending the newly predicted output till the required size\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YtsrqyooqkNO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e9A1haRiqkNZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LgHo8VbqkNn"
   },
   "source": [
    "## STEP 1 : Load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5sQE6BKqkNp"
   },
   "outputs": [],
   "source": [
    "text = open('01_nlp_text_gen_shakespeare_data.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "itR1WjbLqkOE",
    "outputId": "be4623b2-9536-4db4-80a1-ebf807f36c6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_ecVnruqkOl"
   },
   "outputs": [],
   "source": [
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "B0pP0abpqkO-",
    "outputId": "148dfab1-442c-465c-ce88-9bcabaa648d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      "                     2\n",
      "  When forty winters shall besiege thy brow,\n",
      "  And dig deep trenches in thy beauty's field,\n",
      "  Thy youth's proud livery so gazed on now,\n",
      "  Will be a tattered weed of small worth held:  \n",
      "  Then being asked, where all thy beauty lies,\n",
      "  Where all the treasure of thy lusty days;\n",
      "  To say within thine own deep su\n"
     ]
    }
   ],
   "source": [
    "#sample text\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-f7fbsNqkPZ"
   },
   "source": [
    "#### number of encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDX0n6mDqkPd"
   },
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rMKov0RqkPs"
   },
   "outputs": [],
   "source": [
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ae11zoyWqkQD",
    "outputId": "406f9173-6667-42eb-f8b0-60ef8bb47f92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmUbJa9RqkQb"
   },
   "source": [
    "## STEP 2: Text processing - vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qDM0d5r7qkQq"
   },
   "source": [
    "#### Vectorizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6n9ORkQVqkQ2"
   },
   "outputs": [],
   "source": [
    "char_to_ind = {char: ind for ind, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPIG1S7xqkRG"
   },
   "outputs": [],
   "source": [
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "A3c1zOe9qkRe",
    "outputId": "5e8c5d0a-2f67-4904-ee6f-ef0bf01b6fa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " '(': 6,\n",
       " ')': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " '0': 11,\n",
       " '1': 12,\n",
       " '2': 13,\n",
       " '3': 14,\n",
       " '4': 15,\n",
       " '5': 16,\n",
       " '6': 17,\n",
       " '7': 18,\n",
       " '8': 19,\n",
       " '9': 20,\n",
       " ':': 21,\n",
       " ';': 22,\n",
       " '<': 23,\n",
       " '>': 24,\n",
       " '?': 25,\n",
       " 'A': 26,\n",
       " 'B': 27,\n",
       " 'C': 28,\n",
       " 'D': 29,\n",
       " 'E': 30,\n",
       " 'F': 31,\n",
       " 'G': 32,\n",
       " 'H': 33,\n",
       " 'I': 34,\n",
       " 'J': 35,\n",
       " 'K': 36,\n",
       " 'L': 37,\n",
       " 'M': 38,\n",
       " 'N': 39,\n",
       " 'O': 40,\n",
       " 'P': 41,\n",
       " 'Q': 42,\n",
       " 'R': 43,\n",
       " 'S': 44,\n",
       " 'T': 45,\n",
       " 'U': 46,\n",
       " 'V': 47,\n",
       " 'W': 48,\n",
       " 'X': 49,\n",
       " 'Y': 50,\n",
       " 'Z': 51,\n",
       " '[': 52,\n",
       " ']': 53,\n",
       " '_': 54,\n",
       " '`': 55,\n",
       " 'a': 56,\n",
       " 'b': 57,\n",
       " 'c': 58,\n",
       " 'd': 59,\n",
       " 'e': 60,\n",
       " 'f': 61,\n",
       " 'g': 62,\n",
       " 'h': 63,\n",
       " 'i': 64,\n",
       " 'j': 65,\n",
       " 'k': 66,\n",
       " 'l': 67,\n",
       " 'm': 68,\n",
       " 'n': 69,\n",
       " 'o': 70,\n",
       " 'p': 71,\n",
       " 'q': 72,\n",
       " 'r': 73,\n",
       " 's': 74,\n",
       " 't': 75,\n",
       " 'u': 76,\n",
       " 'v': 77,\n",
       " 'w': 78,\n",
       " 'x': 79,\n",
       " 'y': 80,\n",
       " 'z': 81,\n",
       " '|': 82,\n",
       " '}': 83}"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "rVqPOE-aqkRu",
    "outputId": "ad9a2a1b-1041-4ef1-c52f-7b3e760614ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1',\n",
       "       '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?',\n",
       "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
       "       '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\n",
       "       'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v',\n",
       "       'w', 'x', 'y', 'z', '|', '}'], dtype='<U1')"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WqSsV2Q5qkR1",
    "outputId": "3905de3f-f451-4323-9190-37a803722717"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 's')"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind['s'], ind_to_char[74]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dcyxel7SqkSf"
   },
   "source": [
    "#### Encode entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MnMs874uqkSj"
   },
   "outputs": [],
   "source": [
    "encoded_txt = np.array([char_to_ind[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Vi6ONcVWqkSq",
    "outputId": "ed009b9e-506f-40ec-ec18-c029643f4b01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1, ..., 30, 39, 29])"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVCnS1rRqkTD"
   },
   "source": [
    "#### Note : There is a '\\n' character at the begining of the text block - length can be 1 char less "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XzV-oXRUqkTE",
    "outputId": "0e1d6176-6986-40ff-b4a1-3e0c077e0680"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_txt_len = len(encoded_txt)\n",
    "encoded_txt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "O2Fb_1b3qkTb",
    "outputId": "efd390c1-c90d-41de-e1e2-c7c142331ec4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, 12,  0,  1,  1, 31, 73, 70, 68,  1, 61, 56, 64,\n",
       "       73, 60, 74, 75,  1, 58, 73, 60, 56, 75, 76, 73, 60, 74,  1, 78, 60,\n",
       "        1, 59, 60, 74, 64, 73, 60,  1, 64, 69, 58, 73, 60, 56, 74, 60,  8,\n",
       "        0,  1,  1, 45, 63, 56, 75,  1, 75, 63, 60, 73, 60, 57, 80,  1, 57,\n",
       "       60, 56, 76, 75, 80,  5, 74,  1, 73, 70, 74, 60,  1, 68, 64])"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_txt[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQaYwiMRqkTj"
   },
   "source": [
    "## Step 3: Creating batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "83eL3iMlqkTk"
   },
   "source": [
    "#### Observe pattern for choosing a sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "U7V5tF5fqkTl",
    "outputId": "e6206490-1773-42d0-96fc-4ecfdd302c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZT8KkC66qkTw"
   },
   "source": [
    "#### There is a pattern in ending words in every alternate lines\n",
    "* increase-decease, eyes-lies, fuel - cruel\n",
    "* Taking three lines as a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4A9nNYztqkTx"
   },
   "outputs": [],
   "source": [
    "stanza = \"\"\"From fairest creatures we desire increase,\n",
    "  That thereby beauty's rose might never die,\n",
    "  But as the riper should by time decease,\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bWCT4dJxqkUS",
    "outputId": "c7ccbd8d-5a7a-4b9d-fc81-1db7ed7833c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stanza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BpOUYpsIqkUd"
   },
   "source": [
    "#### Taking sequence of 120 characters for estimating the next characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-iadEUfbqkUr"
   },
   "outputs": [],
   "source": [
    "seq_len = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0J5pKAgMqkU9"
   },
   "source": [
    "#### Create a batch of size 120 characters from text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIB6Q48CqkU-"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ulRNx8tLqkVj",
    "outputId": "800cb177-9a8e-4c6c-dcd4-d54f72d2ba10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "kSarnNlBqkVs",
    "outputId": "bbcd5be9-88a2-4ec7-c6fb-18409013c122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Take forst 4 elements in the encoded text using tensorflow dataset\n",
    "for element in dataset.take(4):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VLJ6nXChqkVx"
   },
   "source": [
    "#### Create i/p and target text sequences  \n",
    "* i/p and target Sequences of size 120\n",
    "* For creating seq size of 120, we need 121 characters outof which 1-120 are for i/p, 2-121 for target\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OfovrjK3qkV1"
   },
   "outputs": [],
   "source": [
    "#create sequences in bacthes of size 120, Dropping last few characters\n",
    "#Combines consecutive elements of this dataset into batches.\n",
    "seq_dataset = dataset.batch(batch_size = seq_len +1 , drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eMcx2Lj_qkWW",
    "outputId": "5a4dbcb6-a793-49f2-ebf7-5ac10a0880e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45005"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_seq = encoded_txt_len//(seq_len+1)\n",
    "num_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xEnHnxFLqkWl",
    "outputId": "a4f371e3-16d8-4c90-c469-ceef69d83031"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: (121,), types: tf.int32>"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2VlgNxx2qkWz"
   },
   "source": [
    "#### Create i/p and target sequences from the extracted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KzcsTFh-qkW0"
   },
   "outputs": [],
   "source": [
    "def create_seq(dataset):\n",
    "    ip_seq = dataset[:-1]\n",
    "    target_seq = dataset[1:]\n",
    "    return ip_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-znhqHz4qkXO"
   },
   "outputs": [],
   "source": [
    "sequences = seq_dataset.map(create_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vTu7j8kyqkXh"
   },
   "source": [
    "#### Display two consecutive sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "X8djWpyTqkXj",
    "outputId": "aec90f74-e652-46e6-f409-eb0d3388e1da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
      "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
      "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
      " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
      " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
      "\n",
      "\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But\n",
      "\n",
      "\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
      "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
      " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
      " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
      "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
      "\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But \n",
      "\n",
      "\n",
      "[56 74  1 75 63 60  1 73 64 71 60 73  1 74 63 70 76 67 59  1 57 80  1 75\n",
      " 64 68 60  1 59 60 58 60 56 74 60  8  0  1  1 33 64 74  1 75 60 69 59 60\n",
      " 73  1 63 60 64 73  1 68 64 62 63 75  1 57 60 56 73  1 63 64 74  1 68 60\n",
      " 68 70 73 80 21  0  1  1 27 76 75  1 75 63 70 76  1 58 70 69 75 73 56 58\n",
      " 75 60 59  1 75 70  1 75 63 64 69 60  1 70 78 69  1 57 73 64 62 63 75  1]\n",
      "\n",
      "\n",
      "as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright \n",
      "\n",
      "\n",
      "[74  1 75 63 60  1 73 64 71 60 73  1 74 63 70 76 67 59  1 57 80  1 75 64\n",
      " 68 60  1 59 60 58 60 56 74 60  8  0  1  1 33 64 74  1 75 60 69 59 60 73\n",
      "  1 63 60 64 73  1 68 64 62 63 75  1 57 60 56 73  1 63 64 74  1 68 60 68\n",
      " 70 73 80 21  0  1  1 27 76 75  1 75 63 70 76  1 58 70 69 75 73 56 58 75\n",
      " 60 59  1 75 70  1 75 63 64 69 60  1 70 78 69  1 57 73 64 62 63 75  1 60]\n",
      "\n",
      "\n",
      "s the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright e\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ip, op in sequences.take(2):\n",
    "    #ip\n",
    "    print(ip.numpy())\n",
    "    print('\\n')\n",
    "    print(\"\".join(ind_to_char[ip.numpy()]))\n",
    "    print('\\n')\n",
    "\n",
    "    #op\n",
    "    print(op.numpy())\n",
    "    print('\\n')\n",
    "    print(\"\".join(ind_to_char[op.numpy()]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TjbXHU5fqkXq"
   },
   "source": [
    "#### Shuffle dataset sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_h4EXB4qkXr"
   },
   "outputs": [],
   "source": [
    "buffer_size = 10000 # for internal use\n",
    "sequences = sequences.shuffle(buffer_size = buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "-OE3P1_7qkYM",
    "outputId": "49037f6d-101c-4d5c-f00b-76a4c1e8bf55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 45 63 56 75  5 74  1 68 80  1 57 73 56 77 60  1 57 70 80 10  0  1  1\n",
      " 47 40 37 46 38 39 34 26 10  1 30 77 60 69  1 63 60  8  1 80 70 76 73  1\n",
      " 78 64 61 60  8  1 75 63 64 74  1 67 56 59 80  8  1 56 69 59  1 68 80 74\n",
      " 60 67 61  8  0  1  1  1  1 26 73 60  1 74 76 64 75 70 73 74  1 75 70  1\n",
      " 80 70 76 10  0  1  1 28 40 43 34 40 37 26 39 46 44 10  1 34  1 57 60 74]\n",
      "\n",
      "\n",
      " That's my brave boy.\n",
      "  VOLUMNIA. Even he, your wife, this lady, and myself,\n",
      "    Are suitors to you.\n",
      "  CORIOLANUS. I bes\n",
      "\n",
      "\n",
      "[45 63 56 75  5 74  1 68 80  1 57 73 56 77 60  1 57 70 80 10  0  1  1 47\n",
      " 40 37 46 38 39 34 26 10  1 30 77 60 69  1 63 60  8  1 80 70 76 73  1 78\n",
      " 64 61 60  8  1 75 63 64 74  1 67 56 59 80  8  1 56 69 59  1 68 80 74 60\n",
      " 67 61  8  0  1  1  1  1 26 73 60  1 74 76 64 75 70 73 74  1 75 70  1 80\n",
      " 70 76 10  0  1  1 28 40 43 34 40 37 26 39 46 44 10  1 34  1 57 60 74 60]\n",
      "\n",
      "\n",
      "That's my brave boy.\n",
      "  VOLUMNIA. Even he, your wife, this lady, and myself,\n",
      "    Are suitors to you.\n",
      "  CORIOLANUS. I bese\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ip, op in sequences.take(1):\n",
    "    #ip\n",
    "    print(ip.numpy())\n",
    "    print('\\n')\n",
    "    print(\"\".join(ind_to_char[ip.numpy()]))\n",
    "    print('\\n')\n",
    "\n",
    "    #op\n",
    "    print(op.numpy())\n",
    "    print('\\n')\n",
    "    print(\"\".join(ind_to_char[op.numpy()]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQGyoEu9qkYZ"
   },
   "source": [
    "#### Combine sequences in to batches for training\n",
    "* Taking batches of batch_size = 128 eamples for training\n",
    "* There are total num_seq = 45005 number of sequences\n",
    "* Each sequence of size seq_len = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5mg06S6qkYa"
   },
   "outputs": [],
   "source": [
    "#Combines consecutive elements of this dataset into batches.\n",
    "batch_size = 128 # Number of examples taken for updating weights in traning\n",
    "sequences = sequences.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "owMbKvAFqkYt",
    "outputId": "2f6a44f0-e9fe-4efe-d007-7911620b06bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTDFHbYwqkZC"
   },
   "source": [
    "## Steap 4 : Creating the model\n",
    "#### Recommneded training on GPU or on google colab\n",
    "* Embedding layer with 64 vectors (input vocab = 84, output embedding vector = 64)\n",
    "    * Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "* GRU units 1026\n",
    "* Dense layer of size vocab = 84\n",
    "* Loss function \n",
    "    * sparse_categorical_crossentropy becasuse labels are integrers and not on-hot encoded\n",
    "    * Use parameter \"from_logits = True\" because the exppected output is label not probability distribution\n",
    "* Epochs = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rw8D0so5qkZD"
   },
   "source": [
    "#### Define loss funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqnXRj8GqkZE"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4-IEH7EqkZe"
   },
   "outputs": [],
   "source": [
    "def loss_sparse_cat(y_true, y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jDurR-KJqkZ0"
   },
   "source": [
    "#### Creating model method - with batch size of 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9qaHyUiqkZ6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7i0N_T7GqkaW"
   },
   "outputs": [],
   "source": [
    "#vocab_len = 84\n",
    "#seq_len = 120\n",
    "#batch_size = 128\n",
    "#embed_dim = 64\n",
    "#rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qD9NBJ6qkaw"
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_len, embed_dim, rnn_units, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_len, embed_dim, batch_input_shape = [batch_size, None]))\n",
    "    # return_sequences: Whether to return the last output in the output sequence, or the full sequence.\n",
    "    # stateful: output is fed into following unit\n",
    "    model.add(GRU(rnn_units, return_sequences = True, stateful = True, recurrent_initializer = 'glorot_uniform'))\n",
    "    model.add(Dense(vocab_len))\n",
    "    model.compile(loss = loss_sparse_cat, optimizer = 'adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6Wode0wqkbd"
   },
   "outputs": [],
   "source": [
    "#vocab_len = 84\n",
    "#seq_len = 120\n",
    "#batch_size = 128\n",
    "embed_dim = 64\n",
    "rnn_units = 1026\n",
    "model = create_model(vocab_len, embed_dim, rnn_units, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "HksEJSdvqkb4",
    "outputId": "c5eb9a98-8e62-4dea-cf3c-2e2edf4742eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (128, None, 64)           5376      \n",
      "_________________________________________________________________\n",
      "gru_23 (GRU)                 (128, None, 1026)         3361176   \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (128, None, 84)           86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "96dymFnPqkb-"
   },
   "source": [
    "## Step 5 : Training the model\n",
    "* Since there are more than 3.4 M parameters, recommended to use GPU or google colab \n",
    "* Using google colab for training:\n",
    "    * upload notebook : open googke colab , upload notebook\n",
    "    * upload data file : files - upload - data file\n",
    "    * in the begining of the notebook use: %tensorflow_version 2.x\n",
    "    * make sure GPU is enabled: Edit -> Note book settings\n",
    "    * run all\n",
    " -> GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ABOATn7_qkb_",
    "outputId": "293463bb-81dd-4e73-97a5-68a99a465b3e"
   },
   "outputs": [],
   "source": [
    "#model.fit(sequences, epochs = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2QGOmbo50_Oj"
   },
   "source": [
    "#### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHs5MHwQ1GQP"
   },
   "outputs": [],
   "source": [
    "#model.save('01_nlp_text_gen_shakespeare.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0NOM0shUnkrT"
   },
   "source": [
    "### Check how the model is performing on a single sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anf1LXBG1UtK"
   },
   "source": [
    "#### load model weights and create a model which accepts single batch of input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5QJltwmnkrY"
   },
   "outputs": [],
   "source": [
    "#create a model instance\n",
    "#vocab_len = 84\n",
    "#seq_len = 120\n",
    "batch_size = 1\n",
    "#embed_dim = 64\n",
    "#rnn_units = 1026\n",
    "model = create_model(vocab_len, embed_dim, rnn_units, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8qM5SHG2Gxm"
   },
   "outputs": [],
   "source": [
    "#Load weights from the pre-trained model\n",
    "model.load_weights('01_nlp_text_gen_shakespeare.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQvvDL4ynkrg"
   },
   "outputs": [],
   "source": [
    "#Builds the model based on input shapes received.\n",
    "model.build(tf.TensorShape([batch_size, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "ZhP08OxWnkro",
    "outputId": "c484f5cc-75a9-4f7e-957c-3bca8cef73c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (1, None, 64)             5376      \n",
      "_________________________________________________________________\n",
      "gru_24 (GRU)                 (1, None, 1026)           3361176   \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (1, None, 84)             86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S09tfRTmnkrw"
   },
   "source": [
    "#### Prepare singe input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "toXz3TYgnkrx",
    "outputId": "2ceaa77b-4a9d-44ad-c8fd-40679d749479"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([120])"
      ]
     },
     "execution_count": 754,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ip_batch, op_batch in sequences.take(1):\n",
    "    ex_ip = ip_batch[0]\n",
    "    ex_op = op_batch[0]\n",
    "    \n",
    "ex_ip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Gb-FXUDznkr4",
    "outputId": "0524cbb0-4b4b-43df-ef42-4f8054bd55f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e' 'b' 'e' 'l' 'l' 'i' 'o' 'u' 's' ' ' 't' 'o' ' ' 'h' 'i' 's' ' ' 'a'\n",
      " 'r' 'm' ',' ' ' 'l' 'i' 'e' 's' ' ' 'w' 'h' 'e' 'r' 'e' ' ' 'i' 't' ' '\n",
      " 'f' 'a' 'l' 'l' 's' ',' '\\n' ' ' ' ' ' ' ' ' ' ' ' ' 'R' 'e' 'p' 'u' 'g'\n",
      " 'n' 'a' 'n' 't' ' ' 't' 'o' ' ' 'c' 'o' 'm' 'm' 'a' 'n' 'd' '.' ' ' 'U'\n",
      " 'n' 'e' 'q' 'u' 'a' 'l' ' ' 'm' 'a' 't' 'c' 'h' \"'\" 'd' ',' '\\n' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' 'P' 'y' 'r' 'r' 'h' 'u' 's' ' ' 'a' 't' ' ' 'P' 'r' 'i'\n",
      " 'a' 'm' ' ' 'd' 'r' 'i' 'v' 'e' 's' ',' ' ' 'i']\n"
     ]
    }
   ],
   "source": [
    "#Input text \n",
    "print(ind_to_char[ex_ip])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "wZB5PeKpnkr-",
    "outputId": "cc3eae56-a072-4435-cc77-d47e62ae2dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b' 'e' 'l' 'l' 'i' 'o' 'u' 's' ' ' 't' 'o' ' ' 'h' 'i' 's' ' ' 'a' 'r'\n",
      " 'm' ',' ' ' 'l' 'i' 'e' 's' ' ' 'w' 'h' 'e' 'r' 'e' ' ' 'i' 't' ' ' 'f'\n",
      " 'a' 'l' 'l' 's' ',' '\\n' ' ' ' ' ' ' ' ' ' ' ' ' 'R' 'e' 'p' 'u' 'g' 'n'\n",
      " 'a' 'n' 't' ' ' 't' 'o' ' ' 'c' 'o' 'm' 'm' 'a' 'n' 'd' '.' ' ' 'U' 'n'\n",
      " 'e' 'q' 'u' 'a' 'l' ' ' 'm' 'a' 't' 'c' 'h' \"'\" 'd' ',' '\\n' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' 'P' 'y' 'r' 'r' 'h' 'u' 's' ' ' 'a' 't' ' ' 'P' 'r' 'i' 'a'\n",
      " 'm' ' ' 'd' 'r' 'i' 'v' 'e' 's' ',' ' ' 'i' 'n']\n"
     ]
    }
   ],
   "source": [
    "#True output\n",
    "print(ind_to_char[ex_op])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-YXYvQLnksB"
   },
   "source": [
    "#### Predict output for the given sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "7K11ZGg-nksB",
    "outputId": "c89f4953-0ca4-4687-aa78-0fce689d4a07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=713166, shape=(1, 120), dtype=int32, numpy=\n",
       "array([[60, 57, 60, 67, 67, 64, 70, 76, 74,  1, 75, 70,  1, 63, 64, 74,\n",
       "         1, 56, 73, 68,  8,  1, 67, 64, 60, 74,  1, 78, 63, 60, 73, 60,\n",
       "         1, 64, 75,  1, 61, 56, 67, 67, 74,  8,  0,  1,  1,  1,  1,  1,\n",
       "         1, 43, 60, 71, 76, 62, 69, 56, 69, 75,  1, 75, 70,  1, 58, 70,\n",
       "        68, 68, 56, 69, 59, 10,  1, 46, 69, 60, 72, 76, 56, 67,  1, 68,\n",
       "        56, 75, 58, 63,  5, 59,  8,  0,  1,  1,  1,  1,  1,  1, 41, 80,\n",
       "        73, 73, 63, 76, 74,  1, 56, 75,  1, 41, 73, 64, 56, 68,  1, 59,\n",
       "        73, 64, 77, 60, 74,  8,  1, 64]])>"
      ]
     },
     "execution_count": 757,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reshape the input sequence as per expected model input shape\n",
    "ex_ip_model = tf.expand_dims(ex_ip, axis = 0)\n",
    "ex_ip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "YP1EO8asnksF",
    "outputId": "948b1a20-ac59-4d3f-f18e-5248456d1505"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.35623282,   0.42526245,   1.0287836 , ...,   1.6154257 ,\n",
       "          -1.814107  ,  -2.450402  ],\n",
       "        [  1.1557689 ,   2.2304497 ,   4.386735  , ...,  -0.47502446,\n",
       "         -10.679666  ,  -5.18043   ],\n",
       "        [  3.753955  ,   5.29618   ,   1.3525339 , ...,  -4.1217804 ,\n",
       "         -10.454757  ,  -9.871591  ],\n",
       "        ...,\n",
       "        [ 10.282167  ,  14.723573  ,   2.896478  , ...,  -7.7263856 ,\n",
       "          -5.9493713 ,  -7.7705812 ],\n",
       "        [ -5.94896   ,   2.662288  ,  -6.838647  , ...,  -5.8988333 ,\n",
       "         -13.018035  , -12.699368  ],\n",
       "        [ -7.7277865 ,  -3.082776  ,  -4.9345417 , ...,  -3.2850256 ,\n",
       "         -14.239203  ,  -8.754406  ]]], dtype=float32)"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_pred = model.predict(ex_ip_model)\n",
    "ex_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5nWImzKnksJ"
   },
   "source": [
    "#### Choose top result and form the output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TFbDM6agnksK",
    "outputId": "4aec75e3-5b93-4509-9aeb-18553bfea785"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 120, 84)"
      ]
     },
     "execution_count": 759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WhBAlDKnnksR",
    "outputId": "6da19720-2c37-4ed4-82b2-3074994813d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([120, 84])"
      ]
     },
     "execution_count": 760,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape the prediced output sequence for label selection\n",
    "ex_pred = tf.squeeze(ex_pred)\n",
    "ex_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AM5rRm-FnksT",
    "outputId": "c4519422-ec8e-4e38-c673-a35d195b96c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=713685, shape=(120, 1), dtype=int64, numpy=\n",
       "array([[77],\n",
       "       [ 2],\n",
       "       [74],\n",
       "       [ 9],\n",
       "       [ 5],\n",
       "       [69],\n",
       "       [76],\n",
       "       [74],\n",
       "       [ 0],\n",
       "       [59],\n",
       "       [56],\n",
       "       [ 1],\n",
       "       [75],\n",
       "       [60],\n",
       "       [74],\n",
       "       [ 1],\n",
       "       [38],\n",
       "       [59],\n",
       "       [68],\n",
       "       [10],\n",
       "       [ 0],\n",
       "       [67],\n",
       "       [70],\n",
       "       [66],\n",
       "       [74],\n",
       "       [74],\n",
       "       [75],\n",
       "       [64],\n",
       "       [60],\n",
       "       [73],\n",
       "       [60],\n",
       "       [ 1],\n",
       "       [75],\n",
       "       [75],\n",
       "       [ 1],\n",
       "       [58],\n",
       "       [64],\n",
       "       [73],\n",
       "       [67],\n",
       "       [74],\n",
       "       [ 1],\n",
       "       [ 0],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [26],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [60],\n",
       "       [56],\n",
       "       [67],\n",
       "       [74],\n",
       "       [64],\n",
       "       [64],\n",
       "       [69],\n",
       "       [58],\n",
       "       [67],\n",
       "       [75],\n",
       "       [73],\n",
       "       [ 1],\n",
       "       [68],\n",
       "       [70],\n",
       "       [69],\n",
       "       [68],\n",
       "       [64],\n",
       "       [69],\n",
       "       [59],\n",
       "       [ 8],\n",
       "       [ 0],\n",
       "       [27],\n",
       "       [71],\n",
       "       [74],\n",
       "       [73],\n",
       "       [76],\n",
       "       [64],\n",
       "       [67],\n",
       "       [ 1],\n",
       "       [69],\n",
       "       [60],\n",
       "       [69],\n",
       "       [75],\n",
       "       [60],\n",
       "       [ 8],\n",
       "       [59],\n",
       "       [ 8],\n",
       "       [ 0],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [33],\n",
       "       [26],\n",
       "       [38],\n",
       "       [70],\n",
       "       [73],\n",
       "       [64],\n",
       "       [80],\n",
       "       [76],\n",
       "       [74],\n",
       "       [ 1],\n",
       "       [56],\n",
       "       [74],\n",
       "       [ 1],\n",
       "       [28],\n",
       "       [64],\n",
       "       [70],\n",
       "       [56],\n",
       "       [68],\n",
       "       [ 5],\n",
       "       [61],\n",
       "       [70],\n",
       "       [70],\n",
       "       [77],\n",
       "       [56],\n",
       "       [69],\n",
       "       [ 8],\n",
       "       [ 1],\n",
       "       [68],\n",
       "       [ 5]], dtype=int64)>"
      ]
     },
     "execution_count": 761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Draws samples from a categorical distribution.\n",
    "ex_pred = tf.random.categorical(ex_pred, num_samples=1)\n",
    "ex_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M85af8xsnksZ",
    "outputId": "fb1e3ddc-227f-4953-c20d-42105bb6be7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([120, 1])"
      ]
     },
     "execution_count": 762,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LfSFNa5rnksc"
   },
   "outputs": [],
   "source": [
    "ex_pred = tf.squeeze(ex_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "snjnitNhnksg",
    "outputId": "d6c31e0c-47a8-4b93-e4a9-d7dcc6148f9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([77,  2, 74,  9,  5, 69, 76, 74,  0, 59, 56,  1, 75, 60, 74,  1, 38,\n",
       "       59, 68, 10,  0, 67, 70, 66, 74, 74, 75, 64, 60, 73, 60,  1, 75, 75,\n",
       "        1, 58, 64, 73, 67, 74,  1,  0,  1,  1,  1,  1, 26,  1,  1, 60, 56,\n",
       "       67, 74, 64, 64, 69, 58, 67, 75, 73,  1, 68, 70, 69, 68, 64, 69, 59,\n",
       "        8,  0, 27, 71, 74, 73, 76, 64, 67,  1, 69, 60, 69, 75, 60,  8, 59,\n",
       "        8,  0,  1,  1,  1,  1, 33, 26, 38, 70, 73, 64, 80, 76, 74,  1, 56,\n",
       "       74,  1, 28, 64, 70, 56, 68,  5, 61, 70, 70, 77, 56, 69,  8,  1, 68,\n",
       "        5], dtype=int64)"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "1YlGZsG5nksi",
    "outputId": "11d64e7d-46de-4b6f-967a-7d26fd60b0ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57, 60, 67, 67, 64, 70, 76, 74,  1, 75, 70,  1, 63, 64, 74,  1, 56,\n",
       "       73, 68,  8,  1, 67, 64, 60, 74,  1, 78, 63, 60, 73, 60,  1, 64, 75,\n",
       "        1, 61, 56, 67, 67, 74,  8,  0,  1,  1,  1,  1,  1,  1, 43, 60, 71,\n",
       "       76, 62, 69, 56, 69, 75,  1, 75, 70,  1, 58, 70, 68, 68, 56, 69, 59,\n",
       "       10,  1, 46, 69, 60, 72, 76, 56, 67,  1, 68, 56, 75, 58, 63,  5, 59,\n",
       "        8,  0,  1,  1,  1,  1,  1,  1, 41, 80, 73, 73, 63, 76, 74,  1, 56,\n",
       "       75,  1, 41, 73, 64, 56, 68,  1, 59, 73, 64, 77, 60, 74,  8,  1, 64,\n",
       "       69])"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_op.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "0U244Cvinksk",
    "outputId": "f46f4674-69d5-438b-cd93-e39c641eab49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v!s-'nus\n",
      "da tes Mdm.\n",
      "loksstiere tt cirls \n",
      "    A  ealsiincltr monmind,\n",
      "Bpsruil nente,d,\n",
      "    HAMoriyus as Cioam'foovan, m'\n"
     ]
    }
   ],
   "source": [
    "#predicted output\n",
    "print(\"\".join(ind_to_char[ex_pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "8z7KwEn3nksm",
    "outputId": "45f661a1-0d2c-4d69-ea17-1dae8b5f9d39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bellious to his arm, lies where it falls,\n",
      "      Repugnant to command. Unequal match'd,\n",
      "      Pyrrhus at Priam drives, in\n"
     ]
    }
   ],
   "source": [
    "#expected output\n",
    "print(\"\".join(ind_to_char[ex_op]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8IuzCgdnksp"
   },
   "source": [
    "## Step 6 : Text generation\n",
    "* Load model weights and create a model that accepts single input sequence\n",
    "* Keep on generting next character in the sequence and appending it to the result text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (1, None, 64)             5376      \n",
      "_________________________________________________________________\n",
      "gru_25 (GRU)                 (1, None, 1026)           3361176   \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (1, None, 84)             86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Instantiate model\n",
    "model = create_model(vocab_len, embed_dim, rnn_units, batch_size = 1)\n",
    "model.load_weights('01_nlp_text_gen_shakespeare.h5')\n",
    "model.build(input_shape = tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating each character at time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_seed, gen_size = 500, temperature = 1.0):\n",
    "    \"\"\" \n",
    "    model: Trained Model to Generate Text\n",
    "    start_seed: Intial Seed text in string form\n",
    "    gen_size: Number of characters to generate\n",
    "    temperature : is used to effect probability of next characters. Temperature effects randomness in our resulting text\n",
    "\n",
    "    Basic idea behind this function is to take in some seed text, format it so\n",
    "    that it is in the correct shape for our network, then loop the sequence as\n",
    "    we keep adding our own predicted characters. Similar to our work in the RNN\n",
    "    time series problems.\n",
    "    \"\"\"\n",
    "    text_generated = []\n",
    "    input_eval = [char_to_ind[ch] for ch in start_seed] \n",
    "    input_eval = tf.expand_dims(input_eval, axis = 0) # input shape as per the model requirements\n",
    "    \n",
    "    model.reset_states() # clear hidden states of the network \n",
    "    for i in range(gen_size):\n",
    "        predictions = model.predict(input_eval) # prediction probabilities for each logit\n",
    "        predictions = tf.squeeze( predictions, axis = 0) # since batch size is 1, remove this dimension\n",
    "        predictions = predictions/temperature #use temperature variable to manipulate prediction probabilities\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples = 1)#choose top probability output for each logit\n",
    "        predicted_id = predicted_id[-1, 0].numpy() #collect last logit output\n",
    "        #print(predicted_id)\n",
    "        input_eval = tf.expand_dims([predicted_id], axis = 0)  # Pass the predicted charracter for the next input\n",
    "        text_generated.append(ind_to_char[predicted_id])\n",
    "        #print(text_generated)\n",
    "    \n",
    "    return (start_seed + \"\".join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flowers.\n",
      "  LEONER. My duty in the nige,\n",
      "    Th' other has I his tongue- stopp'd her moved,\n",
      "    As blenting lappingly lies under the\n",
      "    gone with this plaina lik'd, Caesar will so quickly turn, good friend will she take each one that\n",
      "    drown'd-and washes.\n",
      "  SPEED. how thou didst swear to luck growantly\n",
      "    Would stand me diare their bosoms to inherit.\n",
      "  SHRLONT. And thus:\n",
      "    Brow up, and fear no blushing. Let me know her wrath\n",
      "    And say she workn in thee.\n",
      "    Who hath done this\n",
      "    Will speak your skith what he was\n",
      "    As by a forfeit of this, girl; she my sost ink.\n",
      "  SILVIA. There, take them from a ucause of his love,\n",
      "  And flourish'd from me, and more worthiness,\n",
      "    Direction of that lord whereof thou mightst\n",
      "    That I men draw a well-deeplexion.\n",
      "  CLOWN. Good ev'n fooling, who shall shepherd see the jump away\n",
      "\n",
      "                                   Re-enter CHERPITEN\n",
      "\n",
      "  CHIRON. Meen the Clarence to thee; good Master Touch.\n",
      "    The which he fix upon this present,\n",
      "         Into the servi\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"flower\", gen_size = 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good Bye !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "83eL3iMlqkTk",
    "BpOUYpsIqkUd",
    "Rw8D0so5qkZD"
   ],
   "name": "01_nlp_text_gen_shakespeare.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
